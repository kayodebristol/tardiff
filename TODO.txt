To do:
 - lock stdout before writing, so no two processes can open the same output file
 - in tardiff.c: increase cache size passed to BinSort_create
 - document code
 - verify correct handling of large files (>4GB)
   (The theoretical file size limit is 2TB)
 - refactor identify.c so printing of verbose data is moved to tardiffinfo.c
 - current tools do not verify all writes -- they really should!

To fix:
tardiffinfo:
 - current used-file analysis is O(N^2) while it could be O(N log N)
 - current used-file analysis uses O(N) of stack space
tardiffmerge:
 - current used-file analysis is O(N^2) while it could be O(N log N)

Possible new features:
- allow tardiffpatch to accept multiple diff files (which are then first merged)
- support for multithreaded processing (useful for multi-core systems)

Possible file format extensions:
- add checksum to diff files so their consistency can be verified by tardiffinfo
- store original file name in diff files so it does not have to be specified on
  the command line
- it would be nice to be able to read header data without processing the entire
  file, but how can we do this?

Notes to self:
  - In theory the tools can work with files with more than 0xffffffff blocks
    as long as seeking only occurs to the first 0xffffffff blocks in the input,
    but in practice I probably don't want to support this (and at least
    patch_backward() is broken if block indices do not fit in 32 bits).
    I should probably document somewhere that files are limited to 2 TB - 512 B.
